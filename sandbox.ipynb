{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image\n",
    "\n",
    "from layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = './input/train-jpg/'\n",
    "test_path = './input/test-jpg/'\n",
    "train = pd.read_csv('./input/train_v2.csv')\n",
    "test = pd.read_csv('./input/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.iloc[0])\n",
    "print(test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['tags'][0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "Make loop to import all images & store as a numpy array of (3,32,32)'s\n",
    "Save extracted image data somewhere so I don't need to preprocess each time\n",
    "Convert text labels into multi-hot vectors, with vocab as the 17 labels in alphabetical order. 1 = agriculture, 2 = clear, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = [item for i in range(train.shape[0]) for item in train['tags'][i].split()]\n",
    "vocab_ordered = sorted(set(vocab))\n",
    "vocab_dict = {word: index for index, word in enumerate(vocab_ordered)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_inds = [[vocab_dict[word] for word in row.split()] for row in train['tags']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels_words = [set([word for word in row.split()]) for row in train['tags']]\n",
    "labels = mlb.fit_transform(labels_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = np.zeros((1000,3,256,256))\n",
    "for i,image_name in enumerate(train['image_name'][:1000]):\n",
    "    im = Image.open(train_path + image_name + '.jpg')\n",
    "    im = np.array(im)[:,:,:3]\n",
    "    im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "    train_dataset[i,:,:,:] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_dataset)\n",
    "our_labels = torch.from_numpy(labels[:1000])\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(train_data, our_labels)\n",
    "print(train_tensor_dataset[0])\n",
    "loader_train = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 17])\n"
     ]
    }
   ],
   "source": [
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(3),\n",
    "                nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                Flatten(),\n",
    "                nn.Linear(48387,17)\n",
    "              )\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "simple_model.type(gpu_dtype)\n",
    "\n",
    "#check output dimensions before flattening\n",
    "model_gpu = copy.deepcopy(simple_model).type(gpu_dtype)\n",
    "model_gpu.eval()\n",
    "x = torch.randn(10, 3, 256, 256).type(gpu_dtype)\n",
    "x_var = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "scores = model_gpu(x_var)        # Feed it through the model! \n",
    "print(scores.size())\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss().type(gpu_dtype)\n",
    "optimizer = optim.RMSprop(simple_model.parameters(), lr=1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_model.train()\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I'll try making a dataset subclass which does batch pulls of the data into RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea - ONLY change Dataset subclass. Don't want to touch dataloader at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_path = './input/train-jpg/'\n",
    "test_data_path = './input/test-jpg/'\n",
    "\n",
    "train_labels_path ='./input/train_v2.csv' \n",
    "test_labels_path = None #it's not provided\n",
    "\n",
    "#pd.read_csv(train_labels_path)\n",
    "\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#this is the naive implementation which pulls from file every time you get an item. no caching\n",
    "class NaiveDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors. Naive implementation does data preprocessing per 'get_item' call\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "    \n",
    "    Arguments:\n",
    "        data_path (str): path to image folder\n",
    "        labels_path (str): path to csv containing labels per image\n",
    "        num_examples (int): number of examples\n",
    "    \"\"\"\n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return torch.from_numpy(im)\n",
    "    \n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.data_path = data_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_tensor = self.load_image(idx)\n",
    "        target_tensor = self.labels_tensor[idx]\n",
    "        return data_tensor,target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "class DynamicDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors with dynamic loading and buffering\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "    \n",
    "    Precondition - buffer_size must be a multiple of num_examples (relax this later)\n",
    "\n",
    "    Arguments:\n",
    "        data_path (str): path to image folder\n",
    "        labels_path (str): path to csv containing labels per image\n",
    "        num_examples (int): number of examples\n",
    "        buffer_size (int): size of precaching buffer\n",
    "        rand_seed (None/int): if None, go sequentially. If <0, use system clock for seed. If >0, use seed value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        assert num_examples >= buffer_size\n",
    "        assert num_examples % buffer_size == 0\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.num_examples = num_examples\n",
    "        self.buffer_size = buffer_size\n",
    "        self.rand_seed = 0\n",
    "        self.buffer_index = 0\n",
    "        \n",
    "        if rand_seed is None:\n",
    "            self.inds_array = np.arange(num_examples)\n",
    "        elif rand_seed<=0:\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "        elif rand_seed>0:\n",
    "            np.random.seed(rand_seed)\n",
    "            torch.manual_seed(rand_seed)\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data_tensor = np.zeros((buffer_size,3,256,256))\n",
    "        self.backup_buffer = np.zeros(self.data_tensor.shape)\n",
    "        \n",
    "        for i in range(self.buffer_index*buffer_size,self.buffer_index*buffer_size+1):\n",
    "            self.data_tensor[i,:,:,:] = self.load_image(i)\n",
    "        self.data_tensor = torch.from_numpy(self.data_tensor)\n",
    "        \n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return im  \n",
    "    \n",
    "    def fill_buffer(self):\n",
    "        self.backup_buffer = np.zeros((self.buffer_size,3,256,256)) #does this clear the GPU RAM properly? Monitor memory..\n",
    "        self.buffer_index += 1\n",
    "        for i in range(int(self.buffer_size)):\n",
    "            self.backup_buffer[i,:,:,:] = self.load_image(i+self.buffer_index*self.buffer_size)\n",
    "        self.backup_buffer = torch.from_numpy(self.backup_buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index>self.buffer_index*self.buffer_size/2:\n",
    "            self.fill_buffer()\n",
    "        elif index>=self.buffer_index*self.buffer_size:\n",
    "            self.data_tensor = self.backup_buffer #does this do assignment properly w/o causing a GPU/CPU RAM memory leak?\n",
    "        return self.data_tensor[index%self.buffer_size], self.labels_tensor[index%self.buffer_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "#Since dataloaders are created in conjunction with samplers, and because of our RAM constraint when loading data,\n",
    "#We needed to create this helper function to produce a dataloader object with the appropraite sampler. Without this helper\n",
    "#function, there is a risk that the Sampler would not be able to sample random pictures properly\n",
    "def createFastLoaderWithSampler(data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None, batch_size=100):\n",
    "    dynamic_dataset = DynamicDataset(data_path, labels_path,\n",
    "                 num_examples, buffer_size, rand_seed)\n",
    "    return torch.utils.data.DataLoader(dynamic_dataset, batch_size=100, \n",
    "                                       shuffle=(rand_seed is not None), sampler=SequentialSampler(dynamic_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_naive_dataset = NaiveDataset()\n",
    "train_dynamic_loading_dataset = DynamicDataset()\n",
    "loader_fast_train = createFastLoaderWithSampler(num_examples=NUM_TRAIN,buffer_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "simple_model.train()\n",
    "\n",
    "#loader_dynamic_train = torch.utils.data.DataLoader(train_dynamic_loading_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_fast_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_utils import createFastLoaderWithSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1, loss = 0.2699\n",
      "t = 2, loss = 0.2656\n",
      "t = 3, loss = 0.2769\n",
      "t = 4, loss = 0.2524\n",
      "t = 5, loss = 0.2582\n",
      "t = 6, loss = 0.2780\n",
      "t = 7, loss = 0.2382\n",
      "t = 8, loss = 0.2767\n",
      "t = 9, loss = 0.2550\n",
      "t = 10, loss = 0.2497\n",
      "t = 11, loss = 0.2414\n",
      "t = 12, loss = 0.2589\n",
      "t = 13, loss = 0.2431\n",
      "t = 14, loss = 0.2660\n",
      "t = 15, loss = 0.2521\n",
      "t = 16, loss = 0.2740\n",
      "t = 17, loss = 0.2566\n",
      "t = 18, loss = 0.2724\n",
      "t = 19, loss = 0.2599\n",
      "t = 20, loss = 0.2503\n",
      "t = 21, loss = 0.2935\n",
      "t = 22, loss = 0.2610\n",
      "t = 23, loss = 0.2608\n",
      "t = 24, loss = 0.2747\n",
      "t = 25, loss = 0.2630\n",
      "t = 26, loss = 0.2357\n",
      "t = 27, loss = 0.2982\n",
      "t = 28, loss = 0.2687\n",
      "t = 29, loss = 0.2615\n",
      "t = 30, loss = 0.2539\n",
      "t = 31, loss = 0.2636\n",
      "t = 32, loss = 0.2699\n",
      "t = 33, loss = 0.2552\n",
      "t = 34, loss = 0.2583\n",
      "t = 35, loss = 0.2610\n",
      "t = 36, loss = 0.2748\n",
      "t = 37, loss = 0.2641\n",
      "t = 38, loss = 0.2572\n",
      "t = 39, loss = 0.2420\n",
      "t = 40, loss = 0.2756\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 40000\n",
    "BUFFER_SIZE = 4000\n",
    "\n",
    "loader_fast_train = createFastLoaderWithSampler(num_examples=NUM_TRAIN,buffer_size=BUFFER_SIZE)\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_fast_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
