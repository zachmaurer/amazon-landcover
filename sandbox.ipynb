{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image\n",
    "\n",
    "from layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = './input/train-jpg/'\n",
    "test_path = './input/test-jpg/'\n",
    "train = pd.read_csv('./input/train_v2.csv')\n",
    "test = pd.read_csv('./input/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 2)\n",
      "(61191, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_name         train_0\n",
      "tags          haze primary\n",
      "Name: 0, dtype: object\n",
      "image_name                                  test_0\n",
      "tags          primary clear agriculture road water\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[0])\n",
    "print(test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_5</td>\n",
       "      <td>haze primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_6</td>\n",
       "      <td>agriculture clear cultivation primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_7</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_8</td>\n",
       "      <td>agriculture clear cultivation primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_9</td>\n",
       "      <td>agriculture clear cultivation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                         tags\n",
       "0    train_0                                 haze primary\n",
       "1    train_1              agriculture clear primary water\n",
       "2    train_2                                clear primary\n",
       "3    train_3                                clear primary\n",
       "4    train_4    agriculture clear habitation primary road\n",
       "5    train_5                           haze primary water\n",
       "6    train_6  agriculture clear cultivation primary water\n",
       "7    train_7                                 haze primary\n",
       "8    train_8        agriculture clear cultivation primary\n",
       "9    train_9   agriculture clear cultivation primary road"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haze', 'primary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tags'][0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "Make loop to import all images & store as a numpy array of (3,32,32)'s\n",
    "Save extracted image data somewhere so I don't need to preprocess each time\n",
    "Convert text labels into multi-hot vectors, with vocab as the 17 labels in alphabetical order. 1 = agriculture, 2 = clear, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 0,\n",
       " 'artisinal_mine': 1,\n",
       " 'bare_ground': 2,\n",
       " 'blooming': 3,\n",
       " 'blow_down': 4,\n",
       " 'clear': 5,\n",
       " 'cloudy': 6,\n",
       " 'conventional_mine': 7,\n",
       " 'cultivation': 8,\n",
       " 'habitation': 9,\n",
       " 'haze': 10,\n",
       " 'partly_cloudy': 11,\n",
       " 'primary': 12,\n",
       " 'road': 13,\n",
       " 'selective_logging': 14,\n",
       " 'slash_burn': 15,\n",
       " 'water': 16}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [item for i in range(train.shape[0]) for item in train['tags'][i].split()]\n",
    "vocab_ordered = sorted(set(vocab))\n",
    "vocab_dict = {word: index for index, word in enumerate(vocab_ordered)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_inds = [[vocab_dict[word] for word in row.split()] for row in train['tags']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels_words = [set([word for word in row.split()]) for row in train['tags']]\n",
    "labels = mlb.fit_transform(labels_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = np.zeros((1000,3,256,256))\n",
    "for i,image_name in enumerate(train['image_name'][:1000]):\n",
    "    im = Image.open(train_path + image_name + '.jpg')\n",
    "    im = np.array(im)[:,:,:3]\n",
    "    im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "    train_dataset[i,:,:,:] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3, 256, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "( 0 ,.,.) = \n",
      "  158  143  150  ...   146  153  163\n",
      "  146  153  160  ...   151  164  148\n",
      "  151  165  148  ...   160  146  152\n",
      "      ...         â‹±        ...      \n",
      "  147  156  166  ...   149  160  146\n",
      "  152  161  145  ...   160  141  150\n",
      "  166  147  153  ...   149  156  171\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  148  157  168  ...   147  159  145\n",
      "  150  160  144  ...   161  141  149\n",
      "  166  147  153  ...   149  154  169\n",
      "      ...         â‹±        ...      \n",
      "  148  159  142  ...   167  143  152\n",
      "  160  144  158  ...   146  150  162\n",
      "  146  151  160  ...   145  157  139\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  146  158  140  ...   166  146  151\n",
      "  159  144  158  ...   146  150  164\n",
      "  146  151  164  ...   143  156  137\n",
      "      ...         â‹±        ...      \n",
      "  157  147  149  ...   140  147  153\n",
      "  140  145  153  ...   156  162  151\n",
      "  157  162  149  ...   172  153  157\n",
      "[torch.DoubleTensor of size 3x256x256]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 17]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.from_numpy(train_dataset)\n",
    "our_labels = torch.from_numpy(labels[:1000])\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(train_data, our_labels)\n",
    "print(train_tensor_dataset[0])\n",
    "loader_train = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 17])\n"
     ]
    }
   ],
   "source": [
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(3),\n",
    "                nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                Flatten(),\n",
    "                nn.Linear(48387,17)\n",
    "              )\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "simple_model.type(gpu_dtype)\n",
    "\n",
    "#check output dimensions before flattening\n",
    "model_gpu = copy.deepcopy(simple_model).type(gpu_dtype)\n",
    "model_gpu.eval()\n",
    "x = torch.randn(10, 3, 256, 256).type(gpu_dtype)\n",
    "x_var = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "scores = model_gpu(x_var)        # Feed it through the model! \n",
    "print(scores.size())\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss().type(gpu_dtype)\n",
    "optimizer = optim.RMSprop(simple_model.parameters(), lr=1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1, loss = 0.7058\n",
      "t = 2, loss = 6.9440\n",
      "t = 3, loss = 3.6909\n",
      "t = 4, loss = 12.1577\n",
      "t = 5, loss = 5.1227\n",
      "t = 6, loss = 5.1815\n",
      "t = 7, loss = 4.4140\n",
      "t = 8, loss = 2.6482\n",
      "t = 9, loss = 2.6891\n",
      "t = 10, loss = 2.9625\n"
     ]
    }
   ],
   "source": [
    "simple_model.train()\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I'll try making a dataset subclass which does batch pulls of the data into RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea - ONLY change Dataset subclass. Don't want to touch dataloader at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_path = './input/train-jpg/'\n",
    "test_data_path = './input/test-jpg/'\n",
    "\n",
    "train_labels_path ='./input/train_v2.csv' \n",
    "test_labels_path = None #it's not provided\n",
    "\n",
    "#pd.read_csv(train_labels_path)\n",
    "\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#this is the naive implementation which pulls from file every time you get an item. no caching\n",
    "class NaiveDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors. Naive implementation does data preprocessing per 'get_item' call\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "    \n",
    "    Arguments:\n",
    "        data_path (str): path to image folder\n",
    "        labels_path (str): path to csv containing labels per image\n",
    "        num_examples (int): number of examples\n",
    "    \"\"\"\n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return torch.from_numpy(im)\n",
    "    \n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.data_path = data_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_tensor = self.load_image(idx)\n",
    "        target_tensor = self.labels_tensor[idx]\n",
    "        return data_tensor,target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "class DynamicDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors with dynamic loading and buffering\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "    \n",
    "    Precondition - buffer_size must be a multiple of num_examples (relax this later)\n",
    "\n",
    "    Arguments:\n",
    "        data_path (str): path to image folder\n",
    "        labels_path (str): path to csv containing labels per image\n",
    "        num_examples (int): number of examples\n",
    "        buffer_size (int): size of precaching buffer\n",
    "        rand_seed (None/int): if None, go sequentially. If <0, use system clock for seed. If >0, use seed value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        assert num_examples >= buffer_size\n",
    "        assert num_examples % buffer_size == 0\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.num_examples = num_examples\n",
    "        self.buffer_size = buffer_size\n",
    "        self.rand_seed = 0\n",
    "        self.buffer_index = 0\n",
    "        \n",
    "        if rand_seed is None:\n",
    "            self.inds_array = np.arange(num_examples)\n",
    "        elif rand_seed<=0:\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "        elif rand_seed>0:\n",
    "            np.random.seed(rand_seed)\n",
    "            torch.manual_seed(rand_seed)\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data_tensor = np.zeros((buffer_size,3,256,256))\n",
    "        self.backup_buffer = np.zeros(self.data_tensor.shape)\n",
    "        \n",
    "        for i in range(self.buffer_index*buffer_size,self.buffer_index*buffer_size+1):\n",
    "            self.data_tensor[i,:,:,:] = self.load_image(i)\n",
    "        self.data_tensor = torch.from_numpy(self.data_tensor)\n",
    "        \n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return im  \n",
    "    \n",
    "    def fill_buffer(self):\n",
    "        self.backup_buffer = np.zeros((self.buffer_size,3,256,256)) #does this clear the GPU RAM properly? Monitor memory..\n",
    "        self.buffer_index += 1\n",
    "        for i in range(int(self.buffer_size)):\n",
    "            self.backup_buffer[i,:,:,:] = self.load_image(i+self.buffer_index*self.buffer_size)\n",
    "        self.backup_buffer = torch.from_numpy(self.backup_buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index>self.buffer_index*self.buffer_size/2:\n",
    "            self.fill_buffer()\n",
    "        elif index>=self.buffer_index*self.buffer_size:\n",
    "            self.data_tensor = self.backup_buffer #does this do assignment properly w/o causing a GPU/CPU RAM memory leak?\n",
    "        return self.data_tensor[index%self.buffer_size], self.labels_tensor[index%self.buffer_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "#Since dataloaders are created in conjunction with samplers, and because of our RAM constraint when loading data,\n",
    "#We needed to create this helper function to produce a dataloader object with the appropraite sampler. Without this helper\n",
    "#function, there is a risk that the Sampler would not be able to sample random pictures properly\n",
    "def createFastLoaderWithSampler(data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None, batch_size=100):\n",
    "    dynamic_dataset = DynamicDataset(data_path, labels_path,\n",
    "                 num_examples, buffer_size, rand_seed)\n",
    "    return torch.utils.data.DataLoader(dynamic_dataset, batch_size=100, \n",
    "                                       shuffle=(rand_seed is not None), sampler=SequentialSampler(dynamic_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_naive_dataset = NaiveDataset(num_examples=2000)\n",
    "train_dynamic_loading_dataset = DynamicDataset()\n",
    "loader_naive_train = DataLoader(train_naive_dataset, batch_size=100,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1, loss = 0.2865\n",
      "t = 2, loss = 0.3354\n",
      "t = 3, loss = 0.2689\n",
      "t = 4, loss = 0.2511\n",
      "t = 5, loss = 0.2372\n",
      "t = 6, loss = 0.2993\n",
      "t = 7, loss = 0.3771\n",
      "t = 8, loss = 0.2668\n",
      "t = 9, loss = 0.3461\n",
      "t = 10, loss = 0.2548\n",
      "t = 11, loss = 0.2781\n",
      "t = 12, loss = 0.2333\n",
      "t = 13, loss = 0.3106\n",
      "t = 14, loss = 0.2612\n",
      "t = 15, loss = 0.2355\n",
      "t = 16, loss = 0.3117\n",
      "t = 17, loss = 0.2764\n",
      "t = 18, loss = 0.3994\n",
      "t = 19, loss = 0.3268\n",
      "t = 20, loss = 0.3256\n",
      "t = 1, loss = 0.2613\n",
      "t = 2, loss = 0.2629\n",
      "t = 3, loss = 0.2951\n",
      "t = 4, loss = 0.2804\n",
      "t = 5, loss = 0.2417\n",
      "t = 6, loss = 0.3429\n",
      "t = 7, loss = 0.2466\n",
      "t = 8, loss = 0.2273\n",
      "t = 9, loss = 0.2541\n",
      "t = 10, loss = 0.2712\n",
      "t = 11, loss = 0.3139\n",
      "t = 12, loss = 0.2494\n",
      "t = 13, loss = 0.4289\n",
      "t = 14, loss = 0.3762\n",
      "t = 15, loss = 0.2689\n",
      "t = 16, loss = 0.2755\n",
      "t = 17, loss = 0.2193\n",
      "t = 18, loss = 0.4387\n",
      "t = 19, loss = 0.2702\n",
      "t = 20, loss = 0.3132\n",
      "t = 1, loss = 0.2780\n",
      "t = 2, loss = 0.3132\n",
      "t = 3, loss = 0.2473\n",
      "t = 4, loss = 0.3316\n",
      "t = 5, loss = 0.2630\n",
      "t = 6, loss = 0.2445\n",
      "t = 7, loss = 0.2889\n",
      "t = 8, loss = 0.3054\n",
      "t = 9, loss = 0.2482\n",
      "t = 10, loss = 0.2711\n",
      "t = 11, loss = 0.2909\n",
      "t = 12, loss = 0.3187\n",
      "t = 13, loss = 0.2618\n",
      "t = 14, loss = 0.3450\n",
      "t = 15, loss = 0.3540\n",
      "t = 16, loss = 0.3159\n",
      "t = 17, loss = 0.2563\n",
      "t = 18, loss = 0.2449\n",
      "t = 19, loss = 0.2805\n",
      "t = 20, loss = 0.3132\n",
      "t = 1, loss = 0.2447\n",
      "t = 2, loss = 0.3384\n",
      "t = 3, loss = 0.3513\n",
      "t = 4, loss = 0.2697\n",
      "t = 5, loss = 0.2821\n",
      "t = 6, loss = 0.2607\n",
      "t = 7, loss = 0.3128\n",
      "t = 8, loss = 0.2840\n",
      "t = 9, loss = 0.2439\n",
      "t = 10, loss = 0.3084\n",
      "t = 11, loss = 0.2864\n",
      "t = 12, loss = 0.2680\n",
      "t = 13, loss = 0.2363\n",
      "t = 14, loss = 0.2639\n",
      "t = 15, loss = 0.2864\n",
      "t = 16, loss = 0.3005\n",
      "t = 17, loss = 0.2848\n",
      "t = 18, loss = 0.2831\n",
      "t = 19, loss = 0.3263\n",
      "t = 20, loss = 0.2482\n",
      "1 loops, best of 3: 3.39 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "simple_model.train()\n",
    "\n",
    "train_naive_dataset = NaiveDataset(num_examples=2000)\n",
    "loader_naive_train = torch.utils.data.DataLoader(train_naive_dataset, batch_size=100, shuffle=True,num_workers=8)\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_naive_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_utils import createFastLoaderWithSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "t = 1, loss = 0.2680\n",
      "t = 2, loss = 0.2649\n",
      "t = 3, loss = 0.3026\n",
      "t = 4, loss = 0.3373\n",
      "t = 5, loss = 0.2515\n",
      "t = 6, loss = 0.3438\n",
      "t = 7, loss = 0.2337\n",
      "t = 8, loss = 0.3320\n",
      "t = 9, loss = 0.2483\n",
      "t = 10, loss = 0.3133\n",
      "t = 11, loss = 0.2506\n",
      "t = 12, loss = 0.2638\n",
      "t = 13, loss = 0.2906\n",
      "t = 14, loss = 0.2933\n",
      "t = 15, loss = 0.2777\n",
      "t = 16, loss = 0.3015\n",
      "t = 17, loss = 0.3176\n",
      "t = 18, loss = 0.3330\n",
      "t = 19, loss = 0.2560\n",
      "t = 20, loss = 0.3259\n",
      "2000\n",
      "t = 1, loss = 0.2681\n",
      "t = 2, loss = 0.2656\n",
      "t = 3, loss = 0.3030\n",
      "t = 4, loss = 0.3245\n",
      "t = 5, loss = 0.2514\n",
      "t = 6, loss = 0.3341\n",
      "t = 7, loss = 0.2306\n",
      "t = 8, loss = 0.3288\n",
      "t = 9, loss = 0.2482\n",
      "t = 10, loss = 0.2482\n",
      "t = 11, loss = 0.2395\n",
      "t = 12, loss = 0.2587\n",
      "t = 13, loss = 0.2413\n",
      "t = 14, loss = 0.3080\n",
      "t = 15, loss = 0.2679\n",
      "t = 16, loss = 0.2779\n",
      "t = 17, loss = 0.3173\n",
      "t = 18, loss = 0.3188\n",
      "t = 19, loss = 0.2754\n",
      "t = 20, loss = 0.3255\n",
      "2000\n",
      "t = 1, loss = 0.2675\n",
      "t = 2, loss = 0.2658\n",
      "t = 3, loss = 0.3032\n",
      "t = 4, loss = 0.2889\n",
      "t = 5, loss = 0.2514\n",
      "t = 6, loss = 0.3336\n",
      "t = 7, loss = 0.2311\n",
      "t = 8, loss = 0.3300\n",
      "t = 9, loss = 0.2482\n",
      "t = 10, loss = 0.2481\n",
      "t = 11, loss = 0.2394\n",
      "t = 12, loss = 0.2589\n",
      "t = 13, loss = 0.2413\n",
      "t = 14, loss = 0.3111\n",
      "t = 15, loss = 0.3290\n",
      "t = 16, loss = 0.2677\n",
      "t = 17, loss = 0.3235\n",
      "t = 18, loss = 0.3822\n",
      "t = 19, loss = 0.2572\n",
      "t = 20, loss = 0.3103\n",
      "2000\n",
      "t = 1, loss = 0.2682\n",
      "t = 2, loss = 0.2658\n",
      "t = 3, loss = 0.3030\n",
      "t = 4, loss = 0.2812\n",
      "t = 5, loss = 0.2525\n",
      "t = 6, loss = 0.3339\n",
      "t = 7, loss = 0.2305\n",
      "t = 8, loss = 0.3259\n",
      "t = 9, loss = 0.2484\n",
      "t = 10, loss = 0.2481\n",
      "t = 11, loss = 0.2395\n",
      "t = 12, loss = 0.2588\n",
      "t = 13, loss = 0.2414\n",
      "t = 14, loss = 0.3070\n",
      "t = 15, loss = 0.3289\n",
      "t = 16, loss = 0.2702\n",
      "t = 17, loss = 0.3164\n",
      "t = 18, loss = 0.3574\n",
      "t = 19, loss = 0.2566\n",
      "t = 20, loss = 0.3103\n",
      "1 loops, best of 3: 11.8 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "NUM_TRAIN = 2000\n",
    "BUFFER_SIZE = 2000\n",
    "\n",
    "loader_fast_train = createFastLoaderWithSampler(num_examples=NUM_TRAIN,buffer_size=BUFFER_SIZE,num_workers=8)\n",
    "#print(len(loader_fast_train))\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_fast_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
