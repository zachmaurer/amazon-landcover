{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image\n",
    "\n",
    "from layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = './input/train-jpg/'\n",
    "test_path = './input/test-jpg/'\n",
    "train = pd.read_csv('./input/train_v2.csv')\n",
    "test = pd.read_csv('./input/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 2)\n",
      "(61191, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_name         train_0\n",
      "tags          haze primary\n",
      "Name: 0, dtype: object\n",
      "image_name                                  test_0\n",
      "tags          primary clear agriculture road water\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[0])\n",
    "print(test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_5</td>\n",
       "      <td>haze primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_6</td>\n",
       "      <td>agriculture clear cultivation primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_7</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_8</td>\n",
       "      <td>agriculture clear cultivation primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_9</td>\n",
       "      <td>agriculture clear cultivation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                         tags\n",
       "0    train_0                                 haze primary\n",
       "1    train_1              agriculture clear primary water\n",
       "2    train_2                                clear primary\n",
       "3    train_3                                clear primary\n",
       "4    train_4    agriculture clear habitation primary road\n",
       "5    train_5                           haze primary water\n",
       "6    train_6  agriculture clear cultivation primary water\n",
       "7    train_7                                 haze primary\n",
       "8    train_8        agriculture clear cultivation primary\n",
       "9    train_9   agriculture clear cultivation primary road"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haze', 'primary']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tags'][0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "Make loop to import all images & store as a numpy array of (3,32,32)'s\n",
    "Save extracted image data somewhere so I don't need to preprocess each time\n",
    "Convert text labels into multi-hot vectors, with vocab as the 17 labels in alphabetical order. 1 = agriculture, 2 = clear, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 0,\n",
       " 'artisinal_mine': 1,\n",
       " 'bare_ground': 2,\n",
       " 'blooming': 3,\n",
       " 'blow_down': 4,\n",
       " 'clear': 5,\n",
       " 'cloudy': 6,\n",
       " 'conventional_mine': 7,\n",
       " 'cultivation': 8,\n",
       " 'habitation': 9,\n",
       " 'haze': 10,\n",
       " 'partly_cloudy': 11,\n",
       " 'primary': 12,\n",
       " 'road': 13,\n",
       " 'selective_logging': 14,\n",
       " 'slash_burn': 15,\n",
       " 'water': 16}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [item for i in range(train.shape[0]) for item in train['tags'][i].split()]\n",
    "vocab_ordered = sorted(set(vocab))\n",
    "vocab_dict = {word: index for index, word in enumerate(vocab_ordered)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_inds = [[vocab_dict[word] for word in row.split()] for row in train['tags']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels_words = [set([word for word in row.split()]) for row in train['tags']]\n",
    "labels = mlb.fit_transform(labels_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = np.zeros((1000,3,256,256))\n",
    "for i,image_name in enumerate(train['image_name'][:1000]):\n",
    "    im = Image.open(train_path + image_name + '.jpg')\n",
    "    im = np.array(im)[:,:,:3]\n",
    "    im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "    train_dataset[i,:,:,:] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3, 256, 256)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "( 0 ,.,.) = \n",
      "  158  143  150  ...   146  153  163\n",
      "  146  153  160  ...   151  164  148\n",
      "  151  165  148  ...   160  146  152\n",
      "      ...         â‹±        ...      \n",
      "  147  156  166  ...   149  160  146\n",
      "  152  161  145  ...   160  141  150\n",
      "  166  147  153  ...   149  156  171\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  148  157  168  ...   147  159  145\n",
      "  150  160  144  ...   161  141  149\n",
      "  166  147  153  ...   149  154  169\n",
      "      ...         â‹±        ...      \n",
      "  148  159  142  ...   167  143  152\n",
      "  160  144  158  ...   146  150  162\n",
      "  146  151  160  ...   145  157  139\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  146  158  140  ...   166  146  151\n",
      "  159  144  158  ...   146  150  164\n",
      "  146  151  164  ...   143  156  137\n",
      "      ...         â‹±        ...      \n",
      "  157  147  149  ...   140  147  153\n",
      "  140  145  153  ...   156  162  151\n",
      "  157  162  149  ...   172  153  157\n",
      "[torch.DoubleTensor of size 3x256x256]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 17]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.from_numpy(train_dataset)\n",
    "our_labels = torch.from_numpy(labels[:1000])\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(train_data, our_labels)\n",
    "print(train_tensor_dataset[0])\n",
    "loader_train = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 17])\n"
     ]
    }
   ],
   "source": [
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, kernel_size=3, stride=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(3),\n",
    "                nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                Flatten(),\n",
    "                nn.Linear(48387,17)\n",
    "              )\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "simple_model.type(gpu_dtype)\n",
    "\n",
    "#check output dimensions before flattening\n",
    "model_gpu = copy.deepcopy(simple_model).type(gpu_dtype)\n",
    "model_gpu.eval()\n",
    "x = torch.randn(10, 3, 256, 256).type(gpu_dtype)\n",
    "x_var = Variable(x.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "scores = model_gpu(x_var)        # Feed it through the model! \n",
    "print(scores.size())\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss().type(gpu_dtype)\n",
    "optimizer = optim.RMSprop(simple_model.parameters(), lr=1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1, loss = 2.2003\n",
      "t = 2, loss = 3.0249\n",
      "t = 3, loss = 2.5567\n",
      "t = 4, loss = 2.6216\n",
      "t = 5, loss = 3.5789\n",
      "t = 6, loss = 3.5036\n",
      "t = 7, loss = 2.9642\n",
      "t = 8, loss = 2.6343\n",
      "t = 9, loss = 1.9421\n",
      "t = 10, loss = 2.2630\n"
     ]
    }
   ],
   "source": [
    "simple_model.train()\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I'll try making a dataset subclass which does batch pulls of the data into RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea - ONLY change Dataset subclass. Don't want to touch dataloader at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_path = './input/train-jpg/'\n",
    "test_data_path = './input/test-jpg/'\n",
    "\n",
    "train_labels_path ='./input/train_v2.csv' \n",
    "test_labels_path = None #it's not provided\n",
    "\n",
    "#pd.read_csv(train_labels_path)\n",
    "\n",
    "NUM_TRAIN = 32000\n",
    "NUM_VAL = train.shape[0]-NUM_TRAIN\n",
    "NUM_TEST = test.shape[0]\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#this is the naive implementation which pulls from file every time you get an item. no caching\n",
    "class NaiveDataset(Dataset):\n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return torch.from_numpy(im)\n",
    "    \n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        self.num_examples = num_examples\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.data_path = data_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_tensor = self.load_image(idx)\n",
    "        target_tensor = self.labels_tensor[idx]\n",
    "        return data_tensor,target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "#TODO - this is the smart version that intelligently caches ahead so you're pulling from RAM. Speeds up training/testing by 2x\n",
    "#using timeit. \n",
    "# Painful to integrate with the Sampler & DataLoader, so holding off for now\n",
    "class DynamicDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors with dynamic loading\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "    \n",
    "    Precondition - buffer_size must be a multiple of num_examples (relax this later)\n",
    "\n",
    "    Arguments:\n",
    "        data (numpy array): contains sample data.\n",
    "        target (numpy array): contains sample targets (labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        assert num_examples <= self.labels_df.shape[0]\n",
    "        assert num_examples >= buffer_size\n",
    "        assert num_examples % buffer_size == 0\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels_words = [set([word for word in row.split()]) for row in self.labels_df['tags']]\n",
    "        self.labels_tensor = torch.from_numpy(mlb.fit_transform(labels_words))\n",
    "        \n",
    "        self.num_examples = num_examples\n",
    "        self.buffer_size = buffer_size\n",
    "        self.rand_seed = 0\n",
    "        self.buffer_index = 0\n",
    "        \n",
    "        if rand_seed is None:\n",
    "            self.inds_array = np.arange(num_examples)\n",
    "        elif rand_seed<=0:\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "        elif rand_seed>0:\n",
    "            np.random.seed(rand_seed)\n",
    "            torch.manual_seed(rand_seed)\n",
    "            self.inds_array = np.random.permutation(num_examples)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data_tensor = np.zeros((buffer_size,3,256,256))\n",
    "        \n",
    "        for i in range(self.buffer_index*buffer_size,self.buffer_index*buffer_size+1):\n",
    "            self.data_tensor[i,:,:,:] = self.load_image(i)\n",
    "        self.data_tensor = torch.from_numpy(self.data_tensor)\n",
    "        \n",
    "    def load_image(self, idx):\n",
    "        image_name = self.labels_df['image_name'][idx]\n",
    "        im = Image.open(self.data_path + image_name + '.jpg')\n",
    "        im = np.array(im)[:,:,:3]\n",
    "        im = np.reshape(im,(im.shape[2],im.shape[0],im.shape[1]))\n",
    "        return im  \n",
    "    \n",
    "    def fill_buffer(self):\n",
    "        self.buffer_index += 1\n",
    "        for i in range(int(self.buffer_size/2)):\n",
    "            self.data_tensor[i,:,:,:] = self.load_image(i+self.buffer_index*self.buffer_size)\n",
    "        self.data_tensor = torch.from_numpy(self.data_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index>self.buffer_index*self.buffer_size/2:\n",
    "            self.fill_buffer()\n",
    "        return self.data_tensor[index%self.buffer_size], self.labels_tensor[index%self.buffer_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "def createFastLoaderWithSampler(data_path=train_data_path, labels_path=train_labels_path,\n",
    "                 num_examples=1000, buffer_size=1000, rand_seed = None, batch_size=100):\n",
    "    dynamic_dataset = DynamicDataset(data_path, labels_path,\n",
    "                 num_examples, buffer_size, rand_seed)\n",
    "    return torch.utils.data.DataLoader(dynamic_dataset, batch_size=100, \n",
    "                                       shuffle=(rand_seed is not None), sampler=SequentialSampler(dynamic_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_naive_dataset = NaiveDataset()\n",
    "train_dynamic_loading_dataset = DynamicDataset()\n",
    "loader_fast_train = createFastLoaderWithSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "copy from numpy.ndarray to DoubleTensor isn't implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-263-86b15e4988bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"simple_model.train()\\n\\nloader_naive_train = torch.utils.data.DataLoader(train_dynamic_loading_dataset, batch_size=100, shuffle=True)\\n\\nprint_every = 10\\n\\n# Load one batch at a time.\\nfor t, (x, y) in enumerate(loader_fast_train):\\n    x_var = Variable(x.type(gpu_dtype))\\n    y_var = Variable(y.type(gpu_dtype))\\n\\n    # This is the forward pass: predict the scores for each class, for each x in the batch.\\n    scores = simple_model(x_var)\\n    \\n    # Use the correct y values and the predicted y values to compute the loss.\\n    loss = loss_fn(scores, y_var)\\n    \\n    if (t + 1) % print_every == 0:\\n        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\\n\\n    # Zero out all of the gradients for the variables which the optimizer will update.\\n    optimizer.zero_grad()\\n    \\n    # This is the backwards pass: compute the gradient of the loss with respect to each \\n    # parameter of the model.\\n    loss.backward()\\n    \\n    # Actually update the parameters of the model using the gradients computed by the backwards pass.\\n    optimizer.step()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-261-0784d94952bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_index\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-261-0784d94952bd>\u001b[0m in \u001b[0;36mfill_buffer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_index\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: copy from numpy.ndarray to DoubleTensor isn't implemented"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "simple_model.train()\n",
    "\n",
    "loader_naive_train = torch.utils.data.DataLoader(train_dynamic_loading_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "print_every = 10\n",
    "\n",
    "# Load one batch at a time.\n",
    "for t, (x, y) in enumerate(loader_fast_train):\n",
    "    x_var = Variable(x.type(gpu_dtype))\n",
    "    y_var = Variable(y.type(gpu_dtype))\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
