{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Wrap the input tensors in Variables\n",
    "    X_var = Variable(X, requires_grad=True)\n",
    "    y_var = Variable(y)\n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores, and then compute the gradients with a backward pass.               #\n",
    "    ##############################################################################\n",
    "    print(X_var.data.size())\n",
    "    y_pred = model(X_var)\n",
    "    print(y_pred.data.size())\n",
    "    print(y_var.data.size())\n",
    "    loss = torch.sum(y_pred.masked_select(y_var.type(cuda.ByteTensor)).squeeze())\n",
    "    #loss = torch.mean(y_pred.gather(1, y_var.view(-1, 1)).squeeze())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = X_var.grad.data\n",
    "    print(grad.size())\n",
    "    \n",
    "    saliency = grad.abs().max(dim=1)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Config --- \n",
      "num_classes : 17\n",
      "seed : 231\n",
      "batch_size : 20\n",
      "print_every : None\n",
      "use_gpu : True\n",
      "epochs : 10\n",
      "num_train : 10\n",
      "lr : 0.0001\n",
      "dtype : <class 'torch.cuda.FloatTensor'>\n",
      "num_val : 10\n",
      "\n",
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 17])\n",
      "torch.Size([10, 17])\n",
      "torch.Size([10, 3, 224, 224])\n",
      "(3, 224, 224)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-12b35e11b351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3156\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3157\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3158\u001b[1;33m                         **kwargs)\n\u001b[0m\u001b[0;32m   3159\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3160\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1892\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5116\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5118\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5119\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    547\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[0;32m    548\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[1;32m--> 549\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFgAAABSCAYAAADQDhNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA6JJREFUeJztnDFoXWUUx39/W3XIoKAZRAsVLA0dHOxDHAURWod20KFd\ntFLJYnF2E9ycBEGUUEvVQSudIggiOLio9AVErEUIghgRjFW6CJXA3+Hd1JCkfZ/1O+/mez0/eJD7\nvpN3Dz8u974v5+TINkkct/WdwLSTgoNJwcGk4GBScDApOJixgiWdkfSbpO+usy5Jb0halvStpEfq\np9kuJVfwWeDQDdYPA/u61zzw1v9Pa3oYK9j2F8AfNwg5CrznEV8Bd0u6r1aCrVPjHnw/8POG45Xu\nvQTYPcmTSZpndBthZmbm4Nzc3CRPX42lpaXfbc+WxNYQ/AuwZ8PxA917W7C9ACwADAYDD4fDCqef\nPJJ+Ko2tcYtYBJ7tvk08Blyx/WuFz50Kxl7Bkj4AHgfulbQCvALcDmD7beAT4ClgGfgLeD4q2RYZ\nK9j28THrBl6sltGUkTu5YFJwMCk4mBQcTAoOJgUHk4KDScHBpOBgUnAwKTiYFBxMCg4mBQeTgoMp\nEizpkKQfut6Hl7dZPyFpVdI33euF+qm2SUlFYxfwJvAko4rxBUmLtr/fFHrO9qmAHJum5Ap+FFi2\n/aPtv4EPGfVCJAWUCC7te3i6a506L2nPNutImpc0lDRcXV29iXTbo9ZD7mNgr+2Hgc+Ad7cLsr1g\ne2B7MDtb1FbQPCWCx/Y92L5s+2p3eBo4WCe99ikRfAHYJ+lBSXcAxxj1QlxjUy/aEeBSvRTbpqRs\nvybpFPApsAs4Y/uipFeBoe1F4CVJR4A1Ro2CJwJzbgr19W9cjbdOLdkelMTmTi6YFBxMCg4mBQeT\ngoNJwcGk4GBScDApOJgUHEwKDiYFB5OCg0nBwdQq298p6Vy3/rWkvbUTbZWSeRHrZfvDwAHguKQD\nm8JOAn/afgh4HXitdqKtUqtsf5R/C53ngSckqV6a7VKrbH8txvYacAW4p0aCrdPbOAPg6vXG1DTA\n/tLAEsEl4wrWY1Yk7QbuAi5v/qCN4wwkDUvrWjsNScXFxCpl++74ue7nZ4DPnUMxgXpl+3eA9yUt\nMyrbH4tMuiV6K9tLmu9uGc3xX3LvTfCtQm6Vg+lF8Lit905l3BTE7Zi44MKt907lLDeegriFPq7g\nZjvmC6YgbqEPwbfUpMB8yAXTh+DiSYHTQB+CS7beU8PEBXd/zlzfel8CPrJ9cdJ53AzdFMQvgf2S\nViSdHPs7uZOLJR9ywaTgYFJwMCk4mBQcTAoOJgUHk4KD+QciXifcRAvmhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbae237f160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torch import cuda, FloatTensor\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "from resnetfinal import *\n",
    "from utils import *\n",
    "\n",
    "import argparse\n",
    "\n",
    "class Config:\n",
    "  def __init__(self):\n",
    "    self.epochs = 10\n",
    "    self.batch_size = 20\n",
    "    self.lr = 1e-4\n",
    "    self.num_train = 10\n",
    "    self.num_val = 10\n",
    "    self.print_every = None\n",
    "    self.use_gpu = True\n",
    "    self.dtype = cuda.FloatTensor if self.use_gpu else FloatTensor\n",
    "    self.num_classes = 17\n",
    "    self.seed = 231\n",
    "\n",
    "  def __str__(self):\n",
    "    properties = vars(self)\n",
    "    properties = [\"{} : {}\".format(k, str(v)) for k, v in properties.items()]\n",
    "    properties = '\\n'.join(properties)\n",
    "    properties = \"--- Config --- \\n\" + properties + \"\\n\"\n",
    "    return properties\n",
    "\n",
    "config = Config() \n",
    "print(config)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.CenterCrop(224)\n",
    "        ])\n",
    "train_dataset = NaiveDataset(TRAIN_DATA_PATH, TRAIN_LABELS_PATH, num_examples = NUM_TRAIN, transform=transform)\n",
    "train_idx, val_idx = splitIndices(train_dataset, config, shuffle =False)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size = config.batch_size, num_workers = 3, sampler = train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size = config.batch_size, num_workers = 1, sampler = val_sampler)\n",
    "\n",
    "config.train_loader = train_loader\n",
    "config.val_loader = val_loader\n",
    "\n",
    "# Create Model\n",
    "model = ResNet()\n",
    "if config.use_gpu:\n",
    "    model = model.cuda()\n",
    "checkpoint = torch.load('experiments/resnet_conv2/checkpoints/best_model.ckpt')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class_names = ['agriculture',\n",
    "        'artisinal_mine',\n",
    "        'bare_ground',\n",
    "        'blooming',\n",
    "        'blow_down',\n",
    "        'clear',\n",
    "        'cloudy',\n",
    "        'conventional_mine',\n",
    "        'cultivation',\n",
    "        'habitation',\n",
    "        'haze',\n",
    "        'partly_cloudy',\n",
    "        'primary',\n",
    "        'road',\n",
    "        'selective_logging',\n",
    "        'slash_burn',\n",
    "        'water']\n",
    "for t, (x, _, y) in enumerate(train_loader):\n",
    "    #x = x.numpy()\n",
    "    #y = y.numpy()\n",
    "    #print(type(x))\n",
    "    x = x.type(cuda.FloatTensor)\n",
    "    y = y.type(cuda.FloatTensor)\n",
    "    saliency = compute_saliency_maps(x, y, model)\n",
    "    saliency = saliency.cpu().numpy()\n",
    "    x = x.cpu().numpy()\n",
    "    N = x.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        print(x[i].shape)\n",
    "        plt.imshow(x[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(\" \".join([class_names[j] for j in y[i] if j != 0]))\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
